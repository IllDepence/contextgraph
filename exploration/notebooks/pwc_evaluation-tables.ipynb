{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5a18cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# import validators\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdceaa4",
   "metadata": {},
   "source": [
    "## evaluation-tables.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42189d20",
   "metadata": {},
   "source": [
    "- sind die Entitäten (task, dataset, paper) die offenbar nur als\n",
    "Strings gegeben sind, eindeutig auf deren URLs (z.B.\n",
    "https://paperswithcode.com/task/image-classification) zurückzuführen?\n",
    "- wie sieht die category-Struktur für die Tasks aus?\n",
    "- gibt es eine Überschneidung zwischen model_name-Angaben und method?\n",
    "- ist die Sub-task-hierarchie eine Baumstruktur oder gibt es Loops o.Ä.?\n",
    "\n",
    "Den Code dazu am besten von Anfang an auch im Repo\n",
    "https://gitlab.hzdr.de/tarek.saier/sc_combi_hiwi\n",
    "dann kann ich im Zweifelsfall auch schon Zwischenergebnisse usw. sehn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e686009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ls3data/datasets/paperswithcode/evaluation-tables.json') as f:\n",
    "    evaluation_tables = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aebe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10886fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categories', 'subtasks', 'source_link', 'task', 'description', 'datasets', 'synonyms'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_tables[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20a5edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Methodology', 'Natural Language Processing', 'Computer Vision']\n"
     ]
    }
   ],
   "source": [
    "# 1. Categories -- what does it mean?\n",
    "# according to the website, the only category for optical character reg is ComVis\n",
    "categories = evaluation_tables[0][\"categories\"]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770ae4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. subtasks\n",
    "# \"substasks\" is a list that contains 8 elements\n",
    "# each element is a dict contains the same keys as each single element in evaluation_tables\n",
    "substasks = evaluation_tables[0][\"subtasks\"]\n",
    "len(substasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fad6af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categories', 'subtasks', 'source_link', 'task', 'description', 'datasets', 'synonyms'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_tables[0][\"subtasks\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7293f4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Active Learning'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the first subtask\n",
    "evaluation_tables[0][\"subtasks\"][0]['task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6087b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the subtask of the first subtask\n",
    "evaluation_tables[0][\"subtasks\"][1][\"subtasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac7f18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. source link\n",
    "# no info about source link (same for subtasks)\n",
    "source_link = evaluation_tables[0][\"source_link\"]\n",
    "source_link == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08860c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optical Character Recognition'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. task\n",
    "task = evaluation_tables[0][\"task\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac7fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars...) or from subtitle text superimposed on an image (for example: from a television broadcast)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. description\n",
    "description = evaluation_tables[0][\"description\"]\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6369bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "\n",
      "{'dataset_links': [{'url': 'https://paperswithcode.com/sota/optical-character-recognition-on-i2l-140k', 'title': 'Papers with Code Leaderboard URL'}], 'description': '', 'sota': {'metrics': ['BLEU'], 'rows': [{'metrics': {'BLEU': '89.09%'}, 'paper_date': '2018-02-15', 'model_links': [], 'uses_additional_data': False, 'code_links': [{'url': 'https://github.com/untrix/im2latex', 'title': 'untrix/im2latex'}], 'paper_title': 'Teaching Machines to Code: Neural Markup Generation with Visual Attention', 'model_name': 'I2L-NOPOOL', 'paper_url': 'http://arxiv.org/abs/1802.05415v2'}, {'metrics': {'BLEU': '89%'}, 'paper_date': '2018-02-15', 'model_links': [], 'uses_additional_data': False, 'code_links': [{'url': 'https://github.com/untrix/im2latex', 'title': 'untrix/im2latex'}], 'paper_title': 'Teaching Machines to Code: Neural Markup Generation with Visual Attention', 'model_name': 'I2L-STRIPS', 'paper_url': 'http://arxiv.org/abs/1802.05415v2'}]}, 'subdatasets': [], 'dataset_citations': [], 'dataset': 'I2L-140K'}\n",
      "\n",
      "\n",
      "dict_keys(['dataset_links', 'description', 'sota', 'subdatasets', 'dataset_citations', 'dataset'])\n"
     ]
    }
   ],
   "source": [
    "# 6. datasets\n",
    "datasets = evaluation_tables[0][\"datasets\"]\n",
    "print(len(datasets))\n",
    "print(\"\\n\")\n",
    "print(datasets[0])\n",
    "print(\"\\n\")\n",
    "print(datasets[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c5bd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://paperswithcode.com/sota/optical-character-recognition-on-im2latex-90k',\n",
       "  'title': 'Papers with Code Leaderboard URL'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[2][\"dataset_links\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0e0eeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im2latex-90k'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[2][\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0630842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. synonyms\n",
    "# all \"synonyms\" are empty\n",
    "evaluation_tables[0]['synonyms']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec6b81",
   "metadata": {},
   "source": [
    "### 1. Does every entity (task, dataset, paper), given as a str, has a specific URL respectively?\n",
    "- z.B. https://paperswithcode.com/task/image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887101f5",
   "metadata": {},
   "source": [
    "#### 1.1 Tasks and Subtasks (and subtasks of subtasks etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c32622b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optical-character-recognition'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. task\n",
    "task = evaluation_tables[0][\"task\"]\n",
    "task.lower().replace(\" \", \"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "339c9e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_no_url = []\n",
    "subtasks_no_url = []\n",
    "prefix = \"https://paperswithcode.com/task/\"\n",
    "\n",
    "def url_not_valid(prefix, surfix):\n",
    "    status = requests.get(prefix+surfix.lower().replace(\" \", \"-\")).status_code\n",
    "    if status == 404:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5cc4729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1280/1280 [11:00<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# This only considers Task and their direct Subtasks, but no sub-subtasks -- needs further correction\n",
    "for element in tqdm(evaluation_tables):\n",
    "    task_name = element[\"task\"]    \n",
    "    if url_not_valid(prefix, task_name):\n",
    "        tasks_no_url.append(task_name)\n",
    "        \n",
    "    subtasks_list = element[\"subtasks\"]\n",
    "    for subtask in subtasks_list:\n",
    "        subtask_name = subtask[\"task\"]\n",
    "        if url_not_valid(prefix, subtask_name):\n",
    "            subtasks_no_url.append({task_name: subtask_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c248572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7dba8b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subtasks_no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d6f12506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categories', 'subtasks', 'source_link', 'task', 'description', 'datasets', 'synonyms'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_tables[0][\"subtasks\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a02669d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'categories': [],\n",
       "  'subtasks': [],\n",
       "  'source_link': None,\n",
       "  'task': 'Active Object Detection',\n",
       "  'description': 'Active Learning for Object Detection',\n",
       "  'datasets': [{'dataset_links': [{'url': 'https://paperswithcode.com/sota/active-object-detection-on-coco',\n",
       "      'title': 'Papers with Code Leaderboard URL'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['AP'],\n",
       "     'rows': [{'metrics': {'AP': '(7.3, 13.8, 16.9, 19.1, 20.8) on 2% ~ 10%'},\n",
       "       'paper_date': '2021-04-06',\n",
       "       'model_links': [],\n",
       "       'uses_additional_data': False,\n",
       "       'code_links': [{'url': 'https://github.com/yuantn/MI-AOD',\n",
       "         'title': 'yuantn/MI-AOD'}],\n",
       "       'paper_title': 'Multiple instance active learning for object detection',\n",
       "       'model_name': 'RetinaNet',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.02324v1'}]},\n",
       "    'subdatasets': [],\n",
       "    'dataset_citations': [],\n",
       "    'dataset': 'COCO'},\n",
       "   {'dataset_links': [{'url': 'https://paperswithcode.com/sota/active-object-detection-on-pascal-voc-07-12',\n",
       "      'title': 'Papers with Code Leaderboard URL'}],\n",
       "    'description': '',\n",
       "    'sota': {'metrics': ['mAP'],\n",
       "     'rows': [{'metrics': {'mAP': '(47.18, 58.41, 64.02, 67.72, 69.79, 71.07, 72.27) on 5% ~ 20%'},\n",
       "       'paper_date': '2021-04-06',\n",
       "       'model_links': [],\n",
       "       'uses_additional_data': False,\n",
       "       'code_links': [{'url': 'https://github.com/yuantn/MI-AOD',\n",
       "         'title': 'yuantn/MI-AOD'}],\n",
       "       'paper_title': 'Multiple instance active learning for object detection',\n",
       "       'model_name': 'RetinaNet',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.02324v1'},\n",
       "      {'metrics': {'mAP': '(53.62, 62.86, 66.83, 69.33, 70.80, 72.21, 72.84, 73.74, 74.18, 74.91) on 1k ~ 10k'},\n",
       "       'paper_date': '2021-04-06',\n",
       "       'model_links': [],\n",
       "       'uses_additional_data': False,\n",
       "       'code_links': [{'url': 'https://github.com/yuantn/MI-AOD',\n",
       "         'title': 'yuantn/MI-AOD'}],\n",
       "       'paper_title': 'Multiple instance active learning for object detection',\n",
       "       'model_name': 'SSD',\n",
       "       'paper_url': 'https://arxiv.org/abs/2104.02324v1'}]},\n",
       "    'subdatasets': [],\n",
       "    'dataset_citations': [],\n",
       "    'dataset': 'PASCAL VOC 07+12'}],\n",
       "  'synonyms': []}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_tables[0][\"subtasks\"][0][\"subtasks\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
